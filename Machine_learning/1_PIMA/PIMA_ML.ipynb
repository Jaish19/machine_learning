{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled28.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsawMfdOfaeh"
      },
      "source": [
        "# **Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEt91vHDfmpU"
      },
      "source": [
        "# **Pima Indians Diabetes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQcSA_Bp8PGt"
      },
      "source": [
        "PregnanciesNumber of times pregnant\n",
        "\n",
        "GlucosePlasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "\n",
        "BloodPressureDiastolic blood pressure (mm Hg)\n",
        "\n",
        "SkinThicknessTriceps skin fold thickness (mm)\n",
        "\n",
        "Insulin2-Hour serum insulin (mu U/ml)\n",
        "\n",
        "BMIBody mass index (weight in kg/(height in m)^2)\n",
        "\n",
        "DiabetesPedigreeFunctionDiabetes pedigree function\n",
        "\n",
        "AgeAge (years)\n",
        "\n",
        "OutcomeClass variable (0 or 1) 268 of 768 are 1, the others are 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9thnW8_BfWqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af34bd86-61f7-4220-c090-188575ab3e76"
      },
      "source": [
        "!git clone https://github.com/Jaish19/machine_learning.git"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'machine_learning' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfWilZ7H8cpE"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ens470-8ge2"
      },
      "source": [
        "dataset = np.loadtxt(\"machine_learning/Machine_learning/1_PIMA/pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "data = dataset[:,0:8]\n",
        "labels = dataset[:,8]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOWrnXLX8mc_"
      },
      "source": [
        "(trainData, testData, trainLabels, testLabels) = train_test_split(np.array(data),\n",
        "\tnp.array(labels), test_size=0.25, random_state=42)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OE3Aqpw87ts"
      },
      "source": [
        "# **Decision Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZFCXtOXkgqV"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1x5droBZwH8RB4nqE77owjcb3m_aOBgvU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VaRT6hPpPG5"
      },
      "source": [
        "**splitter{“best”, “random”}, default=”best”**\n",
        "\n",
        "The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
        "\n",
        "**max_featuresint, float or {“auto”, “sqrt”, “log2”}, default=None**\n",
        "\n",
        "The number of features to consider when looking for the best split:\n",
        "\n",
        "If int, then consider max_features features at each split.\n",
        "\n",
        "If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n",
        "\n",
        "If “auto”, then max_features=sqrt(n_features).\n",
        "\n",
        "If “sqrt”, then max_features=sqrt(n_features).\n",
        "\n",
        "If “log2”, then max_features=log2(n_features).\n",
        "\n",
        "If None, then max_features=n_features.\n",
        "\n",
        "*Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9eVrLOt85uD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "699fd92d-2f98-4f6a-db6a-246b15558025"
      },
      "source": [
        "#splitter = best, random , max_features = int, auto, log, none\n",
        "model = DecisionTreeClassifier(random_state=84,splitter='best', max_features=8)\n",
        "\n",
        "print(\"[INFO] training model...\")\n",
        "model.fit(trainData, trainLabels)\n",
        "\n",
        "\n",
        "print(\"[INFO] evaluating...\")\n",
        "predictions = model.predict(testData)\n",
        "print(classification_report(testLabels, predictions))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training model...\n",
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.69      0.74       123\n",
            "         1.0       0.55      0.68      0.61        69\n",
            "\n",
            "    accuracy                           0.69       192\n",
            "   macro avg       0.67      0.69      0.67       192\n",
            "weighted avg       0.71      0.69      0.69       192\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMOStwpD9FGs"
      },
      "source": [
        "# **Random Forest**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM9cX7u2p-Is"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1JRk2b-NbpsTbRALKDzIJngfUjOZvj8gk)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKdOjYnG9J-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d8024f-a08d-4279-90b8-743a3d800e8e"
      },
      "source": [
        "model = RandomForestClassifier(n_estimators=10, random_state=42,max_features=1)\n",
        "print(\"[INFO] training model...\")\n",
        "model.fit(trainData, trainLabels)\n",
        "\n",
        "\n",
        "print(\"[INFO] evaluating...\")\n",
        "predictions = model.predict(testData)\n",
        "print(classification_report(testLabels, predictions))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training model...\n",
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.83      0.80       123\n",
            "         1.0       0.66      0.58      0.62        69\n",
            "\n",
            "    accuracy                           0.74       192\n",
            "   macro avg       0.72      0.70      0.71       192\n",
            "weighted avg       0.73      0.74      0.74       192\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tddo9sY9Nn4"
      },
      "source": [
        "# **KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZhe0Tj4sHYX"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=12O0KRMCL0aO9wIebmPoAmfQdHgLBs4Gt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmi2jHsB9Oo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bf3fd73-1667-4463-b813-5312ae4adc5c"
      },
      "source": [
        "#weights = uniform, weights\n",
        "model = KNeighborsClassifier(n_neighbors=9)\n",
        "print(\"[INFO] training model...\")\n",
        "model.fit(trainData, trainLabels)\n",
        "\n",
        "\n",
        "print(\"[INFO] evaluating...\")\n",
        "predictions = model.predict(testData)\n",
        "print(classification_report(testLabels, predictions))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training model...\n",
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.76      0.77       123\n",
            "         1.0       0.60      0.62      0.61        69\n",
            "\n",
            "    accuracy                           0.71       192\n",
            "   macro avg       0.69      0.69      0.69       192\n",
            "weighted avg       0.72      0.71      0.71       192\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWi5XPe39RIr"
      },
      "source": [
        "# **SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U_5BRZGv_DT"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1I48gEU3IMSFRriv-Uv2qVXy1bJqIrBaz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydWqVAuTwFxt"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1qENQ38k3MwqjAyjc19j-zEvbEIfvYI9Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ll2F_6hwzaz"
      },
      "source": [
        "**The C parameter** trades off correct classification of training examples against maximization of the decision function’s margin. For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words``C`` behaves as a regularization parameter in the SVM\n",
        "\n",
        "\n",
        "**The gamma parameter** defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF3IdqNn9T0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c402385f-7712-489a-e6c6-265fac59b202"
      },
      "source": [
        "#kernel='rbf', linear, poly, C=1, gamma='scale'\n",
        "model = SVC(kernel=\"linear\",C=1)\n",
        "print(\"[INFO] training model...\")\n",
        "model.fit(trainData, trainLabels)\n",
        "\n",
        "\n",
        "print(\"[INFO] evaluating...\")\n",
        "predictions = model.predict(testData)\n",
        "print(classification_report(testLabels, predictions))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training model...\n",
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.78      0.79       123\n",
            "         1.0       0.62      0.64      0.63        69\n",
            "\n",
            "    accuracy                           0.73       192\n",
            "   macro avg       0.71      0.71      0.71       192\n",
            "weighted avg       0.73      0.73      0.73       192\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njb9Sy3N9C2G"
      },
      "source": [
        "# **Logistic Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HO6v_XqyjSp"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1N2HJHTUlzSj6jqD8b61pYbCYo_rhXaYS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-4fRG-2yS0F"
      },
      "source": [
        "**solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’**\n",
        "Algorithm to use in the optimization problem.\n",
        "\n",
        "For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n",
        "\n",
        "For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n",
        "\n",
        "‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n",
        "\n",
        "‘liblinear’ and ‘saga’ also handle L1 penalty\n",
        "\n",
        "‘saga’ also supports ‘elasticnet’ penalty\n",
        "\n",
        "‘liblinear’ does not support setting penalty='none'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOsdLirbyM2Y"
      },
      "source": [
        "**penalty{‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’**\n",
        "\n",
        "Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties. ‘elasticnet’ is only supported by the ‘saga’ solver. If ‘none’ (not supported by the liblinear solver), no regularization is applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JCc9ND1fryj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b48ead-221a-469c-cc90-382d8668e0c8"
      },
      "source": [
        "#penality = l1,l2, elasticnet\n",
        "#solver = liblinear, sag, saga, lbfgs, newton-cg\n",
        "#max-iter = 100\n",
        "\n",
        "model = LogisticRegression(max_iter=15)#,solver = 'sag')\n",
        "\n",
        "#penalty='l1',solver=\"saga\",max_iter=100\n",
        "\n",
        "print(\"[INFO] training model...\")\n",
        "model.fit(trainData, trainLabels)\n",
        "\n",
        "\n",
        "print(\"[INFO] evaluating...\")\n",
        "predictions = model.predict(testData)\n",
        "print(classification_report(testLabels, predictions))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training model...\n",
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.86      0.77       123\n",
            "         1.0       0.56      0.32      0.41        69\n",
            "\n",
            "    accuracy                           0.67       192\n",
            "   macro avg       0.63      0.59      0.59       192\n",
            "weighted avg       0.65      0.67      0.64       192\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}